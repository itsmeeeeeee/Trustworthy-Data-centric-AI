You're GPT4 and are about to start a task where you will be shown some sentences written by learners of English. Some of these sentences will contain errors, and alongside each sentence you will be shown 4 different possible corrections, and you will be asked to evaluate the quality of the correction based on some metrics defined below. This task is called Grammatical Error Correction (GEC), and is the task of automatically detecting and correcting errors in text. The task not only includes the correction of grammatical errors, such as missing prepositions and mismatched subject-verb agreement, but also orthographic and semantic errors, such as misspellings and word choice errors respectively. Note that not all sentences you will see include grammatical errors; if they do not, we would expect the corrected version to be identical to the source. We ask that you carefully read the original sentence and rank each of the 4 corrections according to the following metrics, which are defined below.

Semantics. This assesses whether the meaning of the text is preserved following the GEC. Semantic preservation is assessed on a 5-point Likert scale from 1 (Meaning Not Preserved) to 5 (Meaning fully preserved). NOTE: You should penalize corrections which change the meaning unnecessarily. For example, the sentence "I wentt at Rome for my birthday" should be corrected to "I went to Rome for my birthday". A correction such as "I went to Rome for my anniversary" should be penalised in this category as it introduces unnecessary changes to the meaning.

Grammaticality. This assesses the quality of the correction and answers the question "How many errors are left in the corrected sentence?". Please provide a count of the remaining errors, regardless of whether they were present in the source or they were newly introduced errors in the supposed corrected version. The options are "0", "1", "2 or more". Note that, unlike for semantics where a score of 5 is better than a score of 1, here a score of "0" is better than a score of "1" which is better than a score of "2 or more" (this is because if there are 0 errors remaining, the GEC task has been fulfilled).

Over-correction. Since there can be multiple ways to correct a sentence, this assesses whether the correction is unnecessarily verbose or makes unnecessary syntax changes. The best correction should be done with the minimum number of edits. For example, if the sentence "I wentt at Rome for my birthday" is corrected to "I decided to go to Rome for my birthday" this should be penalized under this category because it contains unnecessary syntax changes, even though the final sentence is grammatically correct. This metric answers the question: Is the system over-correcting or making unnecessary syntax changes? The answers should be "No", "Minor over-correction", "Moderate over-correction" or "Substantial over-correction".

We will pass you the input you need to rank in json format.
Please reply with the scores in json format.
This is an example json query where "original_input" is the source sentence, "id" is the unique identifier, and all other keys represent the output corrected sentences which you need to evaluate.
{"original_input": "Travalling by car is also much more convenient.", "gold_reference": "Travelling by car is also much more convenient.", "opt-iml-max-30b": "Travalling by car is also much more convenient.", "text-davinci-003": "Traveling by car is also much more convenient.", "gpt-3.5-turbo": "Traveling by car is also much more convenient.", "id": "765d0adf8a8c58e2617a435be0d2451edcfcb782492ac662432da53843607d9a"}

Your answer should contain the id and the scores, for example, using the example given above, if you wish to give gold_reference a semantics score of 5, a grammaticality score of "0", an overcorrection score of "No", and you wish to give opt-iml-max-30b a semantics score of 4, a grammaticality score of "1", an overcorrection score of "Minor over-correction",  and you wish to give text-davinci-003 a semantics score of 3, a grammaticality score of "2 or more", an overcorrection score of "Substantial over-correction", and you wish to give gpt-3.5-turbo a semantics score of 5, a grammaticality score of "0", an overcorrection score of "Moderate over-correction", then you should return the following output (note how the id item needs to be preserved to allow for identification):
{"gold_reference": {"semantics": 5, "grammaticality": "0", "overcorrection": "No"}, "opt-iml-max-30b": {"semantics": 4, "grammaticality": "1", "overcorrection": "Minor over-correction"}, "text-davinci-003": {"semantics": 3, "grammaticality": "2 or more", "overcorrection": "Substantial over-correction"}, "gpt-3.5-turbo": {"semantics": 5, "grammaticality": "0", "overcorrection":  "Moderate over-correction"}, "id": "49f45c17ee34e2aa420ef557d944a6f2df96f5226c2793e4a9d379c517fde9a5"}

Is this clear? Do you have any questions or are you ready to start?
